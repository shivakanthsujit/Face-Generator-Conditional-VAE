{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.934Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zPcP9trU2Yk5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.936Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ebksHQyW3d5Z",
    "outputId": "462dc40f-c676-4b4d-c58e-73f5f448c386"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using %s for computation\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.939Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wepPt_wd44w_"
   },
   "outputs": [],
   "source": [
    "project_dir = ''\n",
    "dataset_dir = project_dir + 'celebA/'\n",
    "images_dir = project_dir + 'images/'\n",
    "model_dir = project_dir + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.941Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hYU3Jp0974Rk"
   },
   "outputs": [],
   "source": [
    "batch_size = 32           # number of inputs in each batch\n",
    "epochs = 2               # times to run the model on complete data\n",
    "image_size = 64\n",
    "hidden_size = 1024        # hidden dimension\n",
    "latent_size = 128          # latent vector dimension\n",
    "lr = 1e-3                 # learning rate\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.944Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "RicyyBhu6Kmc",
    "outputId": "b1699ef5-0949-41c1-e2fc-c1e7f66b002d"
   },
   "outputs": [],
   "source": [
    "# !apt-get install p7zip-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.946Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZOkuwYHO6Otq"
   },
   "outputs": [],
   "source": [
    "# !7z e /celebA/img_align_celeba.zip -o/celebA/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.954Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MJKc8vmGReNC"
   },
   "outputs": [],
   "source": [
    "class CelebDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=None, selected_attr=None, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.images_dir = os.path.join(root_dir, 'images/')\n",
    "        self.attr_dir = os.path.join(root_dir, 'list_attr_celeba.csv')\n",
    "        self.partition_dir = os.path.join(root_dir, 'list_eval_partition.csv')\n",
    "        self.transform = transform\n",
    "        self.selected_attr = selected_attr\n",
    "        self.split = split\n",
    "        self.preprocess()\n",
    "\n",
    "    def preprocess(self):\n",
    "        if self.selected_attr is None:\n",
    "            self.num_attr = 40\n",
    "            self.attr = pd.read_csv(self.attr_dir)\n",
    "        else:\n",
    "            self.num_attr = len(self.selected_attr)\n",
    "            self.image_ids = pd.read_csv(self.attr_dir)[\"image_id\"]\n",
    "            self.attr = pd.read_csv(self.attr_dir)[self.selected_attr]\n",
    "\n",
    "        # self.attr.replace(to_replace=-1, value=0, inplace=True)\n",
    "        if self.split is not None:\n",
    "            partition = pd.read_csv(self.partition_dir)\n",
    "            if self.split == 'train':\n",
    "                self.attr = self.attr[partition.partition == 0]\n",
    "            elif self.split == 'valid':\n",
    "                self.attr = self.attr[partition.partition == 1]\n",
    "            elif self.split == 'test':\n",
    "                self.attr = self.attr[partition.partition == 2]\n",
    "\n",
    "        self.attr = self.attr.values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.image_ids[idx]\n",
    "        image = Image.open(os.path.join(self.root_dir, 'images', name))\n",
    "        img_attr = self.attr[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.956Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gRrURlatdbtG",
    "outputId": "0f59aef8-5b87-4642-8110-2aace2aaad7c"
   },
   "outputs": [],
   "source": [
    "# All the Attributes available in the dataset\n",
    "# all_columns = '5_o_Clock_Shadow\tArched_Eyebrows\tAttractive\tBags_Under_Eyes\tBald\tBangs\tBig_Lips\tBig_Nose\tBlack_Hair\tBlond_Hair\tBlurry\tBrown_Hair\tBushy_Eyebrows\tChubby\tDouble_Chin\tEyeglasses\tGoatee\tGray_Hair\tHeavy_Makeup\tHigh_Cheekbones\tMale\tMouth_Slightly_Open\tMustache\tNarrow_Eyes\tNo_Beard\tOval_Face\tPale_Skin\tPointy_Nose\tReceding_Hairline\tRosy_Cheeks\tSideburns\tSmiling\tStraight_Hair\tWavy_Hair\tWearing_Earrings\tWearing_Hat\tWearing_Lipstick\tWearing_Necklace\tWearing_Necktie\tYoung'\n",
    "\n",
    "columns = 'Black_Hair\tBlond_Hair\tBrown_Hair\tMale\tNo_Beard\tSmiling\tStraight_Hair\tWavy_Hair\tYoung'\n",
    "columns = columns.split('\\t')\n",
    "num_columns = len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.958Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "16Sc43Tz3-P7"
   },
   "outputs": [],
   "source": [
    "train_data = CelebDataset(root_dir=dataset_dir, split='train', selected_attr=columns,\n",
    "                          transform=transforms.Compose([transforms.Resise(64), transforms.ToTensor()]))\n",
    "valid_data = CelebDataset(root_dir=dataset_dir + 'celeb/', split='valid', selected_attr=columns, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_data = CelebDataset(root_dir=dataset_dir + 'celeb/', split='test', selected_attr=columns, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.960Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WbIXPAFP3xJd"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "validloader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.962Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "468o1b-bQ9KF"
   },
   "outputs": [],
   "source": [
    "def show_images(images):\n",
    "    images = torchvision.utils.make_grid(images)\n",
    "    show_image(images)\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img.permute(1, 2, 0), interpolation=\"bicubic\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.965Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5Z46mXzeBnsi"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), 1024, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.967Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "IUSosnTa7VHz",
    "outputId": "62a1966c-2e9e-4c75-c525-b038dbb2ef2a"
   },
   "outputs": [],
   "source": [
    "images, attr = next((iter(trainloader)))\n",
    "show_images(images)\n",
    "images = images.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.969Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OhgladtW8rtV"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, image_dim=image_size, hidden_size=hidden_size, latent_size=latent_size, num_classes=num_columns):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, 4, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 4, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            Flatten(),\n",
    "        )\n",
    "        self.encoder_mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.encoder_logvar = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc3 = nn.Linear(latent_size, latent_size - num_classes)\n",
    "        self.fc4 = nn.Linear(latent_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size - num_classes)\n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(hidden_size, 128, 5, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 5, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 6, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, 6, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def sample(self, log_var, mean):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = self.encoder(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.cat((x, a), 1)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        log_var = self.encoder_logvar(x)\n",
    "        mean = self.encoder_mean(x)\n",
    "        z = self.sample(log_var, mean)\n",
    "\n",
    "        z = self.fc3(z)\n",
    "        z = torch.cat((z, a), 1)\n",
    "        x = self.fc4(z)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x, mean, log_var\n",
    "\n",
    "\n",
    "vae = VAE().to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "vae.load_state_dict(torch.load(\n",
    "    model_dir + \"Conditional-VAE-full-dataset.pt\", map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.973Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "_smzgDI-Cg30",
    "outputId": "380e0123-33f0-42af-823b-e1ea9d7d07d2"
   },
   "outputs": [],
   "source": [
    "vae.train()\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, attr) in tqdm(enumerate(trainloader), total = len(trainloader)):\n",
    "        images = images.to(device)\n",
    "        attr = attr.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_image, mean, log_var = vae(images, attr)\n",
    "        CE = F.binary_cross_entropy(\n",
    "            reconstructed_image, images, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "        a = 0.01\n",
    "        if (i % 3 == 0):\n",
    "            a = 0.5\n",
    "\n",
    "        b = 100.0\n",
    "        if (i % 5 == 0):\n",
    "            b = 1.0\n",
    "\n",
    "        loss = CE + a * KLD\n",
    "        loss.backward()\n",
    "        train_loss.append(loss.item()/len(images)\n",
    "        optimizer.step()\n",
    "        if(i % 2000 == 0):\n",
    "            torch.save(vae.state_dict(), model_dir +\n",
    "                       \"Conditional-VAE-full-dataset.pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if(i % 500 == 0):\n",
    "                print(\"Epoch: %d\" % epoch)\n",
    "                print(\"Test Loss:\")\n",
    "                print(loss.item()/len(images))\n",
    "                print(\"Original Images\")\n",
    "                show_images(images.cpu())\n",
    "                print(\"Reconstructed Images\")\n",
    "                show_images(reconstructed_image.cpu())\n",
    "\n",
    "                valid_loss = 0\n",
    "                for j, (valid_images, _) in enumerate(testloader):\n",
    "                    valid_images = valid_images.to(device)\n",
    "                    valid_reconstructed_image, valid_mean, valid_log_var = vae(valid_images)\n",
    "\n",
    "                    valid_CE = F.binary_cross_entropy(valid_reconstructed_image, valid_images, reduction='sum')\n",
    "                    valid_KLD = -0.5 * torch.sum(1 + valid_log_var - valid_mean.pow(2) - valid_log_var.exp())\n",
    "                    valid_loss = valid_loss + valid_CE + a * valid_KLD\n",
    "\n",
    "                    if(j == len(testloader) - 1:\n",
    "                        print(\"Validation Images\")\n",
    "                        show_images(valid_reconstructed_image.cpu())\n",
    "                        print(\"Validation Loss:\")\n",
    "                        print(valid_loss.item()/len(testloader))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.975Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "ED33F1iP9JBP",
    "outputId": "f6693c0e-020e-4fc4-f651-77b070760e9a"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.985Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PfkDYU17b_ML"
   },
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), model_dir+\"Conditional-VAE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.987Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PInxu3JX7AJc"
   },
   "outputs": [],
   "source": [
    "columns = 'Black_Hair\tBlond_Hair\tBrown_Hair\tMale\tNo_Beard\tSmiling\tStraight_Hair\tWavy_Hair\tYoung'\n",
    "columns = columns.split('\\t')\n",
    "label = torch.FloatTensor([1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.989Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-CSekKdx0UTD"
   },
   "outputs": [],
   "source": [
    "z = torch.randn(128).mul(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.992Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3s1iNZlx9DWU"
   },
   "outputs": [],
   "source": [
    "values = {\n",
    "    'Black_Hair': 1.,\n",
    "    'Blond_Hair': 1.,\n",
    "    'Brown_Hair': -1.,\n",
    "    'Male': -1.,\n",
    "    'No_Beard': 1.,\n",
    "    'Smiling': 1.,\n",
    "    'Straight_Hair': 1.,\n",
    "    'Wavy_Hair': -1.,\n",
    "    'Young': 1.\n",
    "}\n",
    "\n",
    "label = torch.FloatTensor(list(values.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.994Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "dmw1--RxQgWT",
    "outputId": "9332e816-1f9c-4f2d-e2ad-a93340c1c1a3"
   },
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "# input_vector = torch.cat((z, label))\n",
    "with torch.no_grad():\n",
    "    z = z.to(device)\n",
    "    label = label.to(device)\n",
    "    z = vae.fc3(z)\n",
    "    z = torch.cat((z, label))\n",
    "    x = vae.fc4(z)\n",
    "    x = x.reshape(1, -1)\n",
    "    x = vae.decoder(x)\n",
    "    # input_vector = input_vector.unsqueeze(0)\n",
    "    # input_vector = input_vector.to(device)\n",
    "    # print(input_vector)\n",
    "    # x = vae.fc(input_vector)\n",
    "    # print(x)\n",
    "    # x = vae.decoder(x)\n",
    "    show_images(x.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.996Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w2_Drr9gj9DS",
    "outputId": "5b414e1f-f8a6-4541-ab8a-f279e5eda3b5"
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:30.998Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DKwI0oq_Qn5Z",
    "outputId": "369fd11f-599c-4b7e-f1dd-539699daad99"
   },
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "vectors = []\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.to(device)\n",
    "        reconstructed_image, mean, log_var = best(images)\n",
    "        temp = list(zip(labels.tolist(), mean.tolist()))\n",
    "        for x in temp:\n",
    "            vectors.append(x)\n",
    "        if(i % 100 == 0):\n",
    "            show_images(reconstructed_image.cpu())\n",
    "            img_name = images_dir + 'evaluation/noKD1' + \\\n",
    "                str(i).zfill(6) + '.png'\n",
    "            torchvision.utils.save_image(\n",
    "                torchvision.utils.make_grid(reconstructed_image), img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.000Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "NA4BwPLPnNsb",
    "outputId": "eb5f790e-f8e1-48d7-8201-97fb09046889"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(images_dir + 'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.004Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Kx944IZjR6_7",
    "outputId": "80bdeeab-fc8d-4bee-8003-f47734b64a76"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "vae.eval()\n",
    "values = {\n",
    "    'Black_Hair': 1.,\n",
    "    'Blond_Hair': 1.,\n",
    "    'Brown_Hair': 1.,\n",
    "    'Male': 1.,\n",
    "    'No_Beard': 1.,\n",
    "    'Smiling': -1.,\n",
    "    'Straight_Hair': 1.,\n",
    "    'Wavy_Hair': -1.,\n",
    "    'Young': 1.\n",
    "}\n",
    "\n",
    "label = torch.FloatTensor(list(values.values()))\n",
    "eval_imgs = []\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    #  transforms.CenterCrop(1000),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_name = '14.jpg'\n",
    "    image = Image.open(images_dir + 'test/' + img_name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    show_images(image.cpu())\n",
    "\n",
    "    label = label.reshape(1, -1)\n",
    "    label = label.to(device)\n",
    "    reconstructed_image, mean, log_var = vae(image, label)\n",
    "    show_images(reconstructed_image.cpu())\n",
    "    eval_imgs.append(reconstructed_image)\n",
    "    img_name = images_dir + 'test/reconstructednosmile' + img_name\n",
    "    torchvision.utils.save_image(\n",
    "        torchvision.utils.make_grid(reconstructed_image), img_name)\n",
    "\n",
    "    # image = Image.open(images_dir + 'test/4.jpg')\n",
    "    # image = Variable(loader(image))\n",
    "    # image = image.unsqueeze(0)\n",
    "    # image = image.to(device)\n",
    "    # reconstructed_image, mean, log_var = vae(image)\n",
    "    # # show_images(image.cpu())\n",
    "    # # show_images(reconstructed_image.cpu())\n",
    "    # # eval_imgs[1].append(reconstructed_image)\n",
    "    # img_name = images_dir + 'test/reconstructed2' + str(j).zfill(1) + '.png'\n",
    "    # torchvision.utils.save_image(torchvision.utils.make_grid(reconstructed_image), img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.007Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kb27kXBSludZ"
   },
   "outputs": [],
   "source": [
    "# eval_imgs = torch.stack(eval_imgs)\n",
    "eval_imgs = eval_imgs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.009Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "Qpx3xxvnmk82",
    "outputId": "680cb168-0893-45be-e9e8-e7e7468600cf"
   },
   "outputs": [],
   "source": [
    "show_images(eval_imgs.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.011Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "26UsvZ8NRvH7"
   },
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "start = np.array([-1.8611,  0.3629, -0.1625,  0.6801,  1.2033,  1.0312,  0.5436,  1.3066,\n",
    "                  0.2905,  0.1377,  0.5122, -0.1663,  2.3431, -0.0896, -0.5873, -1.4804,\n",
    "                  0.8141, -1.2197,  0.0484,  0.6414, -0.8172, -0.9543, -0.8818, -1.1465,\n",
    "                  0.2720,  1.1792,  1.8410, -0.4715,  1.4380,  0.5139,  1.2099, -0.5012])\n",
    "middle = np.array([-0.4763, -0.4644, -0.3850,  0.6598,  0.9110,  0.4451,  0.4617, -0.0526,\n",
    "                   0.2808,  0.6080,  0.5532, -1.5506, -0.5199,  0.1359,  0.0373,  0.4284,\n",
    "                   -0.4134, -1.7078, -0.0309, -1.0195, -0.3151, -0.5569,  0.2832, -0.9132,\n",
    "                   -1.1339, -1.3196,  2.1297,  0.8122,  0.6849, -0.6710, -0.3507, -0.9001])\n",
    "end = np.array([-1.6239,  0.2496, -1.0690, -0.8745,  0.4133,  2.2452, -0.2385, -0.6532,\n",
    "                0.3818, -0.9425,  0.9404,  1.3901, -0.3327, -0.3719, -0.0365,  0.3240,\n",
    "                0.4928, -0.4988, -1.2228, -0.1638,  0.6093, -0.5264, -1.6963, -0.3718,\n",
    "                2.1971,  0.2166, -0.0821, -0.1722, -0.1896, -1.6610, -0.1497,  1.0655])\n",
    "points = 50\n",
    "linfit = interpolate.interp1d(\n",
    "    [1, points/2, points], np.vstack([start, middle, end]), axis=0)\n",
    "with torch.no_grad():\n",
    "    for i in range(2, points-1):\n",
    "        z = linfit(i)\n",
    "        z = torch.FloatTensor(z)\n",
    "        print(z.shape)\n",
    "        z = z.reshape((-1, 32))\n",
    "        z = z.to(device)\n",
    "        z = vae.fc(z)\n",
    "        generated_images = vae.decoder(z)\n",
    "        generated_images = generated_images.view(-1, 64, 64)\n",
    "        img = generated_images[0].cpu()\n",
    "        plt.imshow(img)\n",
    "        img_name = images_dir + 'interpolate/' + str(i).zfill(3)\n",
    "        plt.savefig(img_name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.012Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BT12egGTA1u2"
   },
   "outputs": [],
   "source": [
    "# m = '-0.106484\t-0.009962\t-0.019561\t-0.008069\t0.006305\t-0.015933\t0.019840\t0.067150\t-0.027852\t0.033752\t-0.023144\t-0.103557\t-0.020161\t0.013926\t-0.017485\t-0.051400\t-0.008751\t-0.042782\t-0.024165\t-0.133409\t-0.030269\t0.002624\t0.012955\t0.073078\t0.025009\t-0.022863\t-0.008983\t0.060253\t-0.023170\t0.030583\t-0.039951\t0.073296'\n",
    "# m = '-0.129811\t-0.024494\t0.004528\t-0.003351\t0.012485\t-0.032028\t0.029316\t0.063780\t-0.031832\t0.020394\t-0.026757\t-0.109279\t-0.050319\t-0.006987\t-0.029990\t-0.054825\t-0.022480\t-0.048136\t-0.018980\t-0.136170\t-0.013889\t-0.001449\t0.033935\t0.058430\t-0.006667\t-0.036061\t0.019455\t0.048937\t-0.009147\t0.017413\t-0.019323\t0.070512'\n",
    "m = '5.665212\t0.582472\t0.629977\t0.606327\t0.676091\t0.872109\t0.620518\t0.688579\t0.609230\t0.607343\t0.544618\t0.473328\t0.643389\t0.590706\t0.584597\t0.541984\t0.649343\t0.534719\t0.539281\t0.486277\t0.567356\t0.586760\t0.627318\t0.705939\t0.652336\t0.535795\t0.626065\t0.665714\t0.550902\t0.598984\t0.573023\t0.617176'\n",
    "m = m.split('\\t')\n",
    "m = [float(i) for i in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.014Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZEyDjewODzPK",
    "outputId": "807a0d5e-b346-44bc-e8ab-c578d90a7395"
   },
   "outputs": [],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.017Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jpl7fr9GBYx1",
    "outputId": "611f1a1b-887a-4af2-a2cc-54b62e783b59"
   },
   "outputs": [],
   "source": [
    "# i = i + 1\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        # z = torch.randn(32, device=device)\n",
    "        # z = torch.FloatTensor(z)\n",
    "        # z = z.reshape((-1, 32))\n",
    "        # z = z.to(device)\n",
    "        # print(z)\n",
    "        z = torch.randn(128, device=device)\n",
    "        z = z.reshape((-1, 128))\n",
    "        print(z)\n",
    "        z = vae.fc(z)\n",
    "        generated_image = vae.decoder(z)\n",
    "        generated_image = generated_image.view(3, 64, 64)\n",
    "        show_images(generated_image.cpu())\n",
    "        img_name = images_dir + 'generated/' + str(i).zfill(3) + '.png'\n",
    "        torchvision.utils.save_image(generated_image, img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "C8ixJCFzttrV",
    "outputId": "5deee894-4feb-4d3a-a1c1-f8634a81ae49"
   },
   "outputs": [],
   "source": [
    "labels, z_vectors = list(zip(*vectors))\n",
    "z_vectors = torch.tensor(z_vectors)\n",
    "# z_mean = torch.mean(torch.tensor(z_vectors), 0)\n",
    "# z_vectors.sub_(z_mean.expand_as(z_vectors))\n",
    "# U, S, V = torch.svd(torch.t(z_vectors))\n",
    "# C = torch.mm(z_vectors, U[:, :2]).tolist()\n",
    "# C = [x + [labels[i]] for i, x in enumerate(C)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.023Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "6JirZ_NYFg7b",
    "outputId": "313b84e8-fd14-478d-ddf5-df52c77868b6"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(z_vectors.numpy())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.027Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "3RK0_szrAsLw",
    "outputId": "fa74ccde-5b36-4fcc-f925-29360cdaf1cb"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.029Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4BYQIMSzNiwJ"
   },
   "outputs": [],
   "source": [
    "df.columns = [str(i) for i in range(32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.032Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "Vnu9IJkmLd_o",
    "outputId": "d989f929-a645-4a57-c679-58a29d96f03e"
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='0', y='1', data=df, fit_reg=False, hue='3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M90SDLsHC0yQ"
   },
   "source": [
    "##9. Saving the model\n",
    "Save the model incase we need to load it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-03T05:07:31.035Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fy2MuZeEtK8g"
   },
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), model_dir+\"Conditional-VAE-new-structure.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Face VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}